---
output: 
  pdf_document:
    citation_package: natbib
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    template: C:\Users\CCONE\Documents\Cesar_Conejo\03-Data_Science\00-DS_Projects\Versionamiento_R\clustering\report\svm-latex-ms.tex
bibliography: master.bib
header-includes:
  -  \usepackage{hyperref}
biblio-style: apsr
title: "Unsupervised Algorithms in machine learning"
thanks: "Template taken from (http://github.com/svmiller). **Corresponding author**: svmille@clemson.edu."
author:
- name: Cesar Conejo Villalobos
  affiliation: Data Scientist
abstract: "This document provides some examples of unsupervised algorithms in machine learning. In these techniques, we need to infer the properties of the observations without the help of an output variable or _supervisor_. We review two methods: k-means and hierarchical clustering. Then we use some data from Kaggle for applying these techniques to produce a customer segmentation. The platform that we use is R. Because of the number of observations, we are going to use a parallel process for improving the execution times."
keywords: "Unsupervised, algorithms, k-means, hierarchical classification, kaggle, R, parallel"
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
# spacing: double
endnote: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE,
                      message=FALSE, warning=FALSE,
                      fig.path='figs/',
                      cache.path = '_cache/',
                      fig.process = function(x) {
                      x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
                      if (file.rename(x, x2)) x2 else x
                      })
```


# Introduction

In the book _The Elements of Statistical Learning_ @xie2013ddrk explains that in the case of unsupervised learning, data usually has a set of $N$ observations $(x_{1}, ..., x_{N})$ of a random vector $X$ having joint density $Pr(X)$. The goal is to infer the properties of this probability density.

The techniques that statistics and machine learning offer us for unsupervised learning are the following:

1) Principal components, multidimensional scaling.
2) Cluster analysis.
3) Mixture modeling.
4) Association rules.
 
In this exercise, we are going to focus on cluster analysis. The basis of our model will be the [Kaggle](https://www.kaggle.com/arjunbhasin2013/ccdata) Credit Card dataset for Clustering. The data are an 8950 x 18 matrix. One variable is categorical and represents the customer ID, the next seventeen are real numbers each representing the behavior of credit cardholders. The goal is to define a marketing strategy based on customer segmentation.


The first aspect we need to solve is to find the number of clusters that we need. @xie2013ddrk says that we can have two scenarios:

1) For data segmentation, the number of clusters is defined as part of the problem, and it is base on the capacity and resources of the company. The goal is to find observations that belong to each proposed group.

2) Determine how the observations belong to natural distinct groupings. In this case, the number of clusters is unknown.

For this exercise, we are going to use scenario 2 and trying to find the number of clusters and the characteristics of each group using the following techniques:

1) k-means
2) Hierarchical clustering.


# Techniques

As mentioned before, the goal of unsupervised algorithms is to get the classes as homogeneous as possible and such that they are sufficiently separated. This goal can be specified numerically from the following property:

Suppose that exist a partition $P = (C_{1},..., C_{K})$ of $\Omega$, where $g_{1},..., g_{K}$ are the cluster center of the classes:

$$g_{k} = \frac{1}{|C_{k}|} \sum_{i \in C_{k} }{x_{i}} $$

Also $g$ is the global center $\frac{1}{N} \sum_{i = 1 }^{N} {x_{i}}$. We also define:

* Total point scatter: $T = \frac{1}{N} \sum_{i = 1 }^{N} {|| x_{i} - g ||}^{2}$.

* Within-cluster point scatter: $W(C) = \frac{1}{N} \sum_{k = 1 }^{K} \sum_{i \in C_{k}} {|| x_{i} - g_{k} ||}^{2}$.

* Between-cluster point scatter: $B(C) = \sum_{k = 1}^{K} \frac{|C_{k}|}{N} {|| g_{k} - g ||}^{2}$


In this case, the algorithm requires $B(C)$ to be maximum and $W(C)$ to be minimum. Since the total point scatter $T$ is fixed, then maximizing $B(C)$ automatically minimizes $W(C)$. Therefore, the two goals (homogeneity within classes and separation between classes) are achieved at the same time by minimizing $W(C)$.


Thus, the goal in the *K-means* method is to find a partition $C$ of $\Omega$. Also, we find some representatives of the classes, such that $W(C)$ is minimal. For determining how many clusters a dataset has, we can use the elbow method.


Furthermore, k-means depend on the choice of the number of clusters. On the other hand, hierarchical clustering methods do not expect such designations. Instead, @xie2013ddrk claims that this method demands the user to specify a measure of dissimilarity between (disjoint) groups of observations, based on the pairwise dissimilarities among the observations.


This method of classification uses a notion of proximity between groups of elements to measure the separation between the classes sought. To do this, the concept of aggregation is introduced, which is nothing more than a dissimilarity between groups of individuals: be $A$, $B$ $\subseteq \Omega$ then the aggregation between $A$ and $B$ is $\delta(A,B)$. Then we have the following agglomerative clustering methods:

* Single linkage: $\delta_{SL}(A, B) = min\{d(x_{i}, d_{j})| x_{i} \in A, x_{j} \in B \}$

* Complete linkage: $\delta_{CL}(A, B) = max\{d(x_{i}, d_{j})| x_{i} \in A, x_{j} \in B \}$

* Average linkage: $\delta_{AL}(A, B) = \frac{1}{|A||B|} \sum_{x_{i} \in A, x_{j} \in B  }   d(x_{i}, d_{j})$

* Ward linkage: $\delta_{Ward}(A, B) = \frac{|A||B|}{|A| + |B|} {|| g_{A} - g_{B} ||}^{2}$

# Analysis

We are going to define the marketing strategy using k-means and hierarchal clustering. But first, we will see the distribution of the data.


## **Exploratory Analysis**

```{r load_true, echo = FALSE}
# Format
options("scipen" = 100, "digits" = 2)

cc_general <- read.table("C:/Users/CCONE/Documents/Cesar_Conejo/03-Data_Science/00-DS_Projects/Versionamiento_R/clustering/data/CC_General.csv", header = TRUE,
                         sep = ",",
                         dec = ".",
                         row.names = 1)

source("C:/Users/CCONE/Documents/Cesar_Conejo/03-Data_Science/00-DS_Projects/Versionamiento_R/clustering/functions/stats.R")
source("C:/Users/CCONE/Documents/Cesar_Conejo/03-Data_Science/00-DS_Projects/Versionamiento_R/clustering/functions/normalize.R")

```

We create the function _**cc_stats()**_ for analyzing some of the characteristics of the dataset such as:

* Number of complete observations.

* Number of `NA` values.

* Mean of complete observations.

* Standard desviation of complete observations.

* Number of outliers observations ($Q3 + 1.5IQR$)

* Minimun value of complete observations.

* Maximun value of complete observations.

* 95 quantile

* Upper limit for the value. (mean + 3 sd)


```{r eval=FALSE}
---
# Basic statistics
# Input: x vector
# Output: Summary of statistics of the input

cc_stats <- function(x){
  
  #NA Values
  nas = sum(is.na(x))
  
  # Vector with complete values
  a = x[!is.na(x)]
  
  # Properties
  
  m   = mean(a)
  min = min(a)
  max = max(a)
  s   = sd(a)
  
  # Stats
  stats <- boxplot.stats(a)
  n     <- stats$n
  out   <- length(stats$out)
  
  Q95 = quantile(a, 0.95)
  UL = m + 3*s
  
  return(c(n     = n,
           nas   = nas,
           Mean  = m,
           StDev = s,
           Q_out = out,   
           Min   = min,
           Max   = max,
           Q   = Q95,
           Upper_Limit = UL))
}
```

Using the function _**apply()**_, we see the statistical characteristics for each of the variables:

```{r}

# Vector with the name of the variable
vars <- c("BALANCE",
          "BALANCE_FREQUENCY",
          "PURCHASES",
          "ONEOFF_PURCHASES",
          "INSTALLMENTS_PURCHASES",          
          "CASH_ADVANCE",
          "PURCHASES_FREQUENCY",
          "ONEOFF_PURCHASES_FREQUENCY",
          "PURCHASES_INSTALLMENTS_FREQUENCY",
          "CASH_ADVANCE_FREQUENCY",
          "CASH_ADVANCE_TRX",
          "PURCHASES_TRX",
          "CREDIT_LIMIT",
          "PAYMENTS",
          "MINIMUM_PAYMENTS",
          "PRC_FULL_PAYMENT",
          "TENURE")
```

```{r}
# Apply the function for each variable
describe_stats <- t(data.frame(apply(cc_general[vars], 2, cc_stats)))
describe_stats

```

First of all, there is only a few values with `NA`. If we want to see if they both happen at the same time, we can do:

```{r}
sum(is.na(cc_general$CREDIT_LIMIT) & is.na(cc_general$MINIMUM_PAYMENTS))
```

As a result, the `NA` values do not occur in the same row. For fixing these unknown values, we can follow three alternatives:

* Remove the cases.

* Fill in the unknowns using some strategy.

* Use tools that handle these types of values.

In this case, the unknown values only represent `r round(sum(is.na(cc_general$CREDIT_LIMIT) | is.na(cc_general$MINIMUM_PAYMENTS))/8950*100,2)`%, so we decide delete that observations.

```{r}
cc_general <- cc_general[-which(is.na(cc_general$CREDIT_LIMIT) 
                              | is.na(cc_general$MINIMUM_PAYMENTS)),]
```

Other thing we see is that the variables are measure in diferent scales. For example _BALANCE FREQUENCY_, _PURCHASES FREQUENCY_, _ONE OFF PURCHASES FREQUENCY_ and  _PURCHASES INSTALLMENTS FREQUENCY_ are measure with a score between 0 and 1. Other values are measure in money units and others in number of transactions. Because there are different units then we should scaling variables. We do that with the function _**normalize()**_:

```{r eval=FALSE}
---
## Normalize
## Input: Numeric vector
## Output: Vector normalized.

normalize <- function(x){
  
  min_x <- min(x)
  max_x <- max(x)
  
  return((x - min_x)/(max_x - min_x))
  
}
```

Then, we apply the function to each variable

```{r}
cc_general_norm     <- data.frame(apply(cc_general[vars], 2, normalize))
```

Finally, we apply _**cc_stats()**_ again for seeing the changes in our data.

```{r}
describe_stats_norm <- t(data.frame(apply(cc_general_norm[vars], 2, cc_stats)))
describe_stats_norm

```

Now, all the variables are in a scale from 0 to 1. Also, 
there is no change in the variance of the variables.


## **K-means**

For applying the _k-means_, we develop the following code. We set the seed $1234$ for reproducibility purposes. 

```{r}
set.seed(1234)
```

First, we need decide the number of cluster. We apply K-means clustering to the data using the following techniques:

* Hartigan-Wong

* MacQueen

* Lloyd

* Forgy


```{r eval = FALSE}

library(snow)

# How many k?

cl <- makeCluster(4, type="SOCK")

ignore <- clusterEvalQ(cl, {library(MASS); NULL}) 

#Hartigan-Wong
results_HW <- lapply(seq(1,20), function(x) kmeans(cc_general_norm,
                                                   centers = x,
                                                   algorithm = "Hartigan-Wong",
                                                   nstart = 20))

variance_HW <- sapply(results_HW, function(results_HW) results_HW$tot.withinss)

# MacQueen
results_MQ <- lapply(seq(1,20), function(x) kmeans(cc_general_norm,
                                                   centers = x,
                                                   algorithm = "MacQueen",
                                                   nstart = 20))

variance_MQ <- sapply(results_MQ, function(results_MQ) results_MQ$tot.withinss)

# Lloyd
results_Ll <- lapply(seq(1,20), function(x) kmeans(cc_general_norm,
                                                   centers = x,
                                                   algorithm = "Lloyd",
                                                   nstart = 20))

variance_Ll <- sapply(results_Ll, function(results_Ll) results_Ll$tot.withinss)

# Forgy
results_Fg <- lapply(seq(1,20), function(x) kmeans(cc_general_norm,
                                                   centers = x,
                                                   algorithm = "Forgy",
                                                   nstart = 20))

variance_Fg <- sapply(results_Fg, function(results_Fg) results_Fg$tot.withinss)

stopCluster(cl)
```

Then, we can observe the total within-cluster sum of squares for K-means clustering for some cluster from 1 to 20. 

```{r eval = FALSE}
plot(variance_HW, 
     col = "red",
     type = "b", 
     xlab = "Number of cluster k",
     ylab = "Sum of squares", 
     main = "Elbow Method: No. of clusters by algorithm")
points(variance_MQ, col = "blue", type = "b")
points(variance_Ll, col = "green", type = "b")
points(variance_Fg, col = "magenta", type = "b")
legend("topright",
       legend = c("Hartigan","MacQueen","Lloyd","Forgy"), 
       col = c("red", "blue", "green", "magenta"), 
       lty = 1, 
       lwd = 1)
```

We can see that the kink occur at $k = 4$, so this is the number of cluster that we propose for the marketing strategy. ![Total within-cluster sum of squares for K-means clustering](C:/Users/CCONE/Documents/Cesar_Conejo/03-Data_Science/00-DS_Projects/Versionamiento_R/clustering/output/graphs/p1_elbow_method_norm.png)

Now, the question that we need to answer is which method for k-means to use. Therefore, we code the following lines that shows the results of the clustering in 4 groups. 

```{r eval = FALSE}
# Which method:

# Hartigan-Wong
results_HW <- lapply(seq(1,20), function(x) kmeans(cc_general_norm, 
                                                   centers = 4,
                                                   nstart = 20,
                                                   algorithm = "Hartigan-Wong"))
betweenss_HW <- sapply(results_HW, function(results_HW) results_HW$betweenss)

# MacQueen
results_MQ <- lapply(seq(1,20), function(x) kmeans(cc_general_norm, 
                                                   centers = 4,
                                                   nstart = 20,
                                                   algorithm = "MacQueen"))
betweenss_MQ <- sapply(results_MQ, function(results_MQ) results_MQ$betweenss)


# Lloyd
results_Ll <- lapply(seq(1,20), function(x) kmeans(cc_general_norm, 
                                                   centers = 4,
                                                   nstart = 20,
                                                   algorithm = "Lloyd"))
betweenss_Ll <- sapply(results_Ll, function(results_Ll) results_Ll$betweenss)


# Forgy
results_Fg <- lapply(seq(1,20), function(x) kmeans(cc_general_norm, 
                                                   centers = 4,
                                                   nstart = 20,
                                                   algorithm = "Forgy"))
betweenss_Fg <- sapply(results_Fg, function(results_Fg) results_Fg$betweenss)



AVG_Between_Class <- data.frame(Method = c("Hartigan-Wong", "MacQueen", 
                                           "Lloyd", "Forgy"),
                     AVG_Between_Class = c(mean(betweenss_HW), mean(betweenss_MQ),
                                           mean(betweenss_Ll),mean(betweenss_Fg))
)

```


```markdown

Method        | Avg between
------------- | -------------
Hartigan-Wong | 3222.460
MacQueen      | 3221.499
Lloyd         | 3220.927
Forgy         | 3221.112

```

Because Lloyd reachs the minimun average between-cluster point scatter, we choose that method. As a result, we code the method in the following way:

```{r}
Cluster_FG <- kmeans(x = cc_general_norm,
                     centers =  4,
                     iter.max = 100,
                     nstart = 20,
                     algorithm = "Lloyd")

```

Finally, we can see some properties of the cluster. For example, the number of observations by cluster is:

```{r}
Cluster_FG$size
```

In this case, we can see that the classes are well balanced. Also, we can see the center of each cluster and realize some interpretations.

```{r}
barplot(t(Cluster_FG$centers),
        main = "Centers by cluster",
        xlab = "Cluster",
        beside = TRUE, 
        col = rainbow(17)
        )
```

## **Hierachical cluster**

Using _**Ward**_ aggregation, we code the method in the following way:

```{r}

model_Ward <- hclust(dist(cc_general_norm), method = "ward.D2")

plot(model_Ward, labels = FALSE)
rect.hclust(model_Ward, k = 4, border = "blue")
```




```{r}
cluster <- cutree(model_Ward, k = 4)
cc_general_norm_cluster <-cbind(cc_general_norm, cluster)
library(rattle)
cc_general_center <- centers.hclust(cc_general_norm,
                                    model_Ward,
                                    nclust = 4,
                                    use.median = FALSE)
```


As in _k-means_, we can see the properties of the centers cluster in the following graph

```{r}

barplot(t(cc_general_center),
        beside = TRUE,
        main = "Cluster Interpretation Hierarchical Classification",
        col = rainbow(17)
        )

```

```{r}


## radar graph
center  <- as.data.frame(cc_general_center)
maximos <- apply(center,2,max)
minimos <- apply(center,2,min)
center  <- rbind(minimos,center)
center  <- rbind(maximos,center)
```


```{r}
library(fmsb)
radarchart(center,
           maxmin = TRUE,
           axistype = 4,
           axislabcol = "slategray4",
           centerzero = FALSE,
           seg = 8,
           cglcol = "gray67",
           pcol=c("green","blue","red", "purple", "black", "brown"),
           plty = 1,
           plwd = 5,
           title = "Comparación de clústeres")


legenda <-legend(1.5,1, legend=c("Cluster 1","Cluster 2","Cluster 3", "Cluster 4"),
                 seg.len=-1.4,
                 title="Clústeres",
                 pch=21, 
                 bty="n" ,lwd=3, y.intersp=1, horiz=FALSE,
                 col=c("green","blue","red", "purple", "black", "brown")
                 )
```


# Conclusion







<!--
# References
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\vspace*{-0.2in}
\noindent
-->
