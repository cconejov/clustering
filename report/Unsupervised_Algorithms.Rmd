---
output: 
  pdf_document:
    citation_package: natbib
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    template: C:\Users\CCONE\Documents\Cesar_Conejo\03-Data_Science\00-DS_Projects\Versionamiento_R\clustering\report\svm-latex-ms.tex
bibliography: master.bib
header-includes:
  -  \usepackage{hyperref}
biblio-style: apsr
title: "Unsupervised Algorithms in machine learning"
thanks: "Template taken from (http://github.com/svmiller). **Corresponding author**: svmille@clemson.edu."
author:
- name: Cesar Conejo Villalobos
  affiliation: Data Scientist
abstract: "This document provides some examples of unsupervised algorithms in machine learning. In these techniques, we need to infer the properties of the observations without the help of an output variable or _supervisor_. We review two methods: k-means and hierarchical clustering. Then we use some data from Kaggle for applying these techniques to produce a customer segmentation. The platform that we use is R. Because of the number of observations, we are going to use a parallel process for improving the execution times."
keywords: "Unsupervised, algorithms, k-means, hierarchical classification, kaggle, R, parallel"
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
# spacing: double
endnote: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE,
                      message=FALSE, warning=FALSE,
                      fig.path='figs/',
                      cache.path = '_cache/',
                      fig.process = function(x) {
                      x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
                      if (file.rename(x, x2)) x2 else x
                      })
```


# Introduction

In the book _The Elements of Statistical Learning_ @xie2013ddrk explains that in the case of unsupervised learning, data usually has a set of $N$ observations $(x_{1}, ..., x_{N})$ of a random vector $X$ having joint density $Pr(X)$. The goal is to infer the properties of this probability density.

The techniques that statistics and machine learning offer us for unsupervised learning are the following:

1) Principal components, multidimensional scaling.
2) Cluster analysis.
3) Mixture modeling.
4) Association rules.
 
In this exercise, we are going to focus on cluster analysis. The basis of our model will be the [Kaggle](https://www.kaggle.com/arjunbhasin2013/ccdata) Credit Card dataset for Clustering. The data are an 8950 x 18 matrix. One variable is categorical and represents the customer ID, the next seventeen are real numbers each representing the behavior of credit cardholders. The goal is to define a marketing strategy based on customer segmentation.


The first aspect we need to solve is to find the number of clusters that we need. @xie2013ddrk says that we can have two scenarios:

1) For data segmentation, the number of clusters is defined as part of the problem, and it is base on the capacity and resources of the company. The goal is to find observations that belong to each proposed group.

2) Determine how the observations belong to natural distinct groupings. In this case, the number of clusters is unknown.

For this exercise, we are going to use scenario 2 and trying to find the number of clusters and the characteristics of each group using the following techniques:

1) k-means
2) Hierarchical clustering.


# Techniques

As mentioned before, the goal of unsupervised algorithms is to get the classes as homogeneous as possible and such that they are sufficiently separated. This goal can be specified numerically from the following property:

Suppose that exist a partition $P = (C_{1},..., C_{K})$ of $\Omega$, where $g_{1},..., g_{K}$ are the cluster center of the classes:

$$g_{k} = \frac{1}{|C_{k}|} \sum_{i \in C_{k} }{x_{i}} $$

Also $g$ is the global center $\frac{1}{N} \sum_{i = 1 }^{N} {x_{i}}$. We also define:

* Total point scatter: $T = \frac{1}{N} \sum_{i = 1 }^{N} {|| x_{i} - g ||}^{2}$.

* Within-cluster point scatter: $W(C) = \frac{1}{N} \sum_{k = 1 }^{K} \sum_{i \in C_{k}} {|| x_{i} - g_{k} ||}^{2}$.

* Between-cluster point scatter: $B(C) = \sum_{k = 1}^{K} \frac{|C_{k}|}{N} {|| g_{k} - g ||}^{2}$


In this case, the algorithm requires $B(C)$ to be maximum and $W(C)$ to be minimum. Since the total point scatter $T$ is fixed, then maximizing $B(C)$ automatically minimizes $W(C)$. Therefore, the two goals (homogeneity within classes and separation between classes) are achieved at the same time by minimizing $W(C)$.


Thus, the goal in the *K-means* method is to find a partition $C$ of $\Omega$. Also, we find some representatives of the classes, such that $W(C)$ is minimal. For determining how many clusters a dataset has, we can use the elbow method.


Furthermore, k-means depend on the choice of the number of clusters. On the other hand, hierarchical clustering methods do not expect such designations. Instead, @xie2013ddrk claims that this method demands the user to specify a measure of dissimilarity between (disjoint) groups of observations, based on the pairwise dissimilarities among the observations.


This method of classification uses a notion of proximity between groups of elements to measure the separation between the classes sought. To do this, the concept of aggregation is introduced, which is nothing more than a dissimilarity between groups of individuals: be $A$, $B$ $\subseteq \Omega$ then the aggregation between $A$ and $B$ is $\delta(A,B)$. Then we have the following agglomerative clustering methods:

* Single linkage: $\delta_{SL}(A, B) = min\{d(x_{i}, d_{j})| x_{i} \in A, x_{j} \in B \}$

* Complete linkage: $\delta_{CL}(A, B) = max\{d(x_{i}, d_{j})| x_{i} \in A, x_{j} \in B \}$

* Average linkage: $\delta_{AL}(A, B) = \frac{1}{|A||B|} \sum_{x_{i} \in A, x_{j} \in B  }   d(x_{i}, d_{j})$

* Ward linkage: $\delta_{Ward}(A, B) = \frac{|A||B|}{|A| + |B|} {|| g_{A} - g_{B} ||}^{2}$

# Analysis

We are going to define the marketing strategy using k-means and hierarchal clustering. But first, we will see the distribution of the data.


## **Exploratory Analysis**

```{r load_true, echo = FALSE}
# Format
options("scipen" = 100, "digits" = 2)

cc_general <- read.table("C:/Users/CCONE/Documents/Cesar_Conejo/03-Data_Science/00-DS_Projects/Versionamiento_R/clustering/data/CC_General.csv", header = TRUE,
                         sep = ",",
                         dec = ".",
                         row.names = 1)

source("C:/Users/CCONE/Documents/Cesar_Conejo/03-Data_Science/00-DS_Projects/Versionamiento_R/clustering/functions/stats.R")
source("C:/Users/CCONE/Documents/Cesar_Conejo/03-Data_Science/00-DS_Projects/Versionamiento_R/clustering/functions/normalize.R")

```

We create the function _**cc_stats()**_ for analyzing some of the characteristics of the dataset such as:

* Number of complete observations.

* Number of `NA` values.

* Mean of complete observations.

* Standard desviation of complete observations.

* Number of outliers observations. ($Q3 + 1.5IQR$)

* Minimun value of complete observations.

* Maximun value of complete observations.

* 95 quantile.

* Upper limit for the value. (mean + 3 sd)


```{r eval=FALSE}
---
# Basic statistics
# Input: x vector
# Output: Summary of statistics of the input

cc_stats <- function(x){
  
  #NA Values
  nas = sum(is.na(x))
  
  # Vector with complete values
  a = x[!is.na(x)]
  
  # Properties
  
  m   = mean(a)
  min = min(a)
  max = max(a)
  s   = sd(a)
  
  # Stats
  stats <- boxplot.stats(a)
  n     <- stats$n
  out   <- length(stats$out)
  
  Q95 = quantile(a, 0.95)
  UL = m + 3*s
  
  return(c(n     = n,
           nas   = nas,
           Mean  = m,
           StDev = s,
           Q_out = out,   
           Min   = min,
           Max   = max,
           Q   = Q95,
           Upper_Limit = UL))
}
```

Using the function _**apply()**_, we see the statistical characteristics for each of the variables:

```{r}

# Vector with the name of variables
vars <- c("BALANCE",
          "BALANCE_FREQUENCY",
          "PURCHASES",
          "ONEOFF_PURCHASES",
          "INSTALLMENTS_PURCHASES",          
          "CASH_ADVANCE",
          "PURCHASES_FREQUENCY",
          "ONEOFF_PURCHASES_FREQUENCY",
          "PURCHASES_INSTALLMENTS_FREQUENCY",
          "CASH_ADVANCE_FREQUENCY",
          "CASH_ADVANCE_TRX",
          "PURCHASES_TRX",
          "CREDIT_LIMIT",
          "PAYMENTS",
          "MINIMUM_PAYMENTS",
          "PRC_FULL_PAYMENT",
          "TENURE")
```

```{r}
# Apply the function for each variable
describe_stats <- t(data.frame(apply(cc_general[vars], 2, cc_stats)))
describe_stats

```

First of all, there is only a few values with `NA`. If we want to see if they both happen at the same time, we can do:

```{r}
sum(is.na(cc_general$CREDIT_LIMIT) & is.na(cc_general$MINIMUM_PAYMENTS))
```

As a result, the `NA` values do not occur in the same row. For fixing these unknown values, we can follow three alternatives:

* Remove the cases.

* Fill in the unknowns using some strategy.

* Use tools that handle these types of values.

In this case, the unknown values only represent `r round(sum(is.na(cc_general$CREDIT_LIMIT) | is.na(cc_general$MINIMUM_PAYMENTS))/8950*100,2)`% of the data, so we decide to delete  the observations.

```{r}
cc_general <- cc_general[-which(is.na(cc_general$CREDIT_LIMIT) 
                              | is.na(cc_general$MINIMUM_PAYMENTS)),]
```

Another aspect we see is that the variables are measure in different scales. For example _BALANCE FREQUENCY_, _PURCHASES FREQUENCY_, _ONE OFF PURCHASES FREQUENCY_ and  _PURCHASES INSTALLMENTS FREQUENCY_ are measure with a score between 0 and 1. Other values are measure in money units and others in the number of transactions. Because there are different units then we should scaling variables. We do that with the function _**normalize()**_:

```{r eval=FALSE}
---
## Normalize
## Input: Numeric vector
## Output: Vector normalized.

normalize <- function(x){
  
  min_x <- min(x)
  max_x <- max(x)
  
  return((x - min_x)/(max_x - min_x))
  
}
```

Then, we apply the function to each variable:

```{r}
cc_general_norm <- data.frame(apply(cc_general[vars], 2, normalize))
```

Finally, we apply _**cc_stats()**_ again for seeing the changes in our data.

```{r}
describe_stats_norm <- t(data.frame(apply(cc_general_norm[vars], 2, cc_stats)))
describe_stats_norm

```

Now, all the variables are on a scale from 0 to 1. Also, there is no change in the variance of the variables because we are only scaling and no standardizing. Finally, we save the new data set for being used as the source in the parallel executions:

```{r eval = FALSE}
#Save table
write.table(x = cc_general_norm,
            file = "output/data/cc_general_norm.csv",
            sep = ",",
            dec = ".")

```


## **K-means**

For applying _k-means_, we develop the following code. We set the seed $1234$ for reproducibility purposes. 

```{r}
set.seed(1234)
```

First, we need to decide the number of clusters. We apply K-means clustering to the data using the following techniques:

* Hartigan-Wong

* MacQueen

* Lloyd

* Forgy

Also, we use the `snow` library for performing parallelizable operations.

```{r eval = FALSE}
library(snow)

cl <- makeCluster(4, type="SOCK")
# Read data in each cluster
ignore <- clusterEvalQ(cl, 
                       { data <- read.csv("output/data/cc_general_norm.csv",
                                           header = TRUE,
                                           sep = ",",
                                           dec = "."
                                          )
                                  set.seed(1234)
                                  NULL}) 

# Hartigan-Wong
results_HW <- clusterApply(cl,
                           seq(1,20),
                           function(x) kmeans(data,
                                              centers = x,
                                              algorithm = "Hartigan-Wong",
                                              nstart = 50))

withinss_HW <-  sapply(results_HW, function(results_HW) results_HW$tot.withinss)

# MacQueen
results_MQ <- clusterApply(cl,
                           seq(1,20),
                           function(x) kmeans(data,
                                              centers = x,
                                              algorithm = "MacQueen",
                                              nstart = 50))

withinss_MQ <-  sapply(results_MQ, function(results_MQ) results_MQ$tot.withinss)

# Lloyd
results_Ll <- clusterApply(cl,
                           seq(1,20),
                           function(x) kmeans(data,
                                              centers = x,
                                              algorithm = "Lloyd",
                                              nstart = 50))

withinss_Ll <-  sapply(results_Ll, function(results_Ll) results_Ll$tot.withinss)


# Forgy
results_FG <- clusterApply(cl,
                           seq(1,20),
                           function(x) kmeans(data,
                                              centers = x,
                                              algorithm = "Forgy",
                                              nstart = 50))

withinss_FG <-  sapply(results_FG, function(results_FG) results_FG$tot.withinss)

# end paralell
stopCluster(cl)
```

Then, we can observe the total within-cluster sum of squares for K-means clustering for some clusters from 1 to 20. 

```{r eval = FALSE}
plot(withinss_HW, 
     col = "red",
     type = "b",
     xlab = "Number of cluster k",
     ylab = "Sum of squares",
     main = "Elbow Method: No. of clusters by algorithm")
points(withinss_MQ, col = "blue",    type = "b")
points(withinss_Ll, col = "green",   type = "b")
points(withinss_FG, col = "magenta", type = "b")
legend("topright",
       legend = c("Hartigan","MacQueen","Lloyd","Forgy"), 
       col = c("red", "blue", "green", "magenta"), 
       lty = 1, 
       lwd = 1)
```
![Total within-cluster sum of squares for K-means clustering](C:/Users/CCONE/Documents/Cesar_Conejo/03-Data_Science/00-DS_Projects/Versionamiento_R/clustering/output/graphs/p1_elbow_method.png){width=400px}


We can see that the kink occurs at $k = 5$, so this is the number of clusters that we propose for the marketing strategy. Now, the question that we need to answer is which method for k-means to use. Therefore, we code the following lines that show the results of the clustering in 5 groups.

```{r eval = FALSE}
# Which method:
cl <- makeCluster(4, type="SOCK")
# Read data in each cluster
ignore <- clusterEvalQ(cl, 
                       { data <- read.csv("output/data/cc_general_norm.csv",
                                          header = TRUE,
                                          sep = ",",
                                          dec = "."
                       )
                       set.seed(1234)
                       NULL}) 

# Evaluate all the posible algorithms
results_Algorithm <- clusterApply(cl,
                           c("Hartigan-Wong","MacQueen","Lloyd","Forgy"),
                           function(algorithm) kmeans(data,
                                              centers = 5,
                                              algorithm = algorithm,
                                              nstart = 50))
for(i in 1:4) print(results_Algorithm[[i]]$betweenss)
# end cluster
stopCluster(cl)

```



```{r load_true2, echo = FALSE}
# Which method:
library(snow)
cl <- makeCluster(4, type="SOCK")
# Read data in each cluster
ignore <- clusterEvalQ(cl, 
                       { data <- read.csv("C:/Users/CCONE/Documents/Cesar_Conejo/03-Data_Science/00-DS_Projects/Versionamiento_R/clustering/output/data/cc_general_norm.csv",
                                          header = TRUE,
                                          sep = ",",
                                          dec = "."
                       )
                       set.seed(1234)
                       NULL}) 

# Evaluate all the posible algorithms
results_Algorithm <- clusterApply(cl,
                           c("Hartigan-Wong","MacQueen","Lloyd","Forgy"),
                           function(algorithm) kmeans(data,
                                              centers = 5,
                                              algorithm = algorithm,
                                              nstart = 50))
for(i in 1:4) print(results_Algorithm[[i]]$betweenss)
# end cluster
stopCluster(cl)
```


Because _Hartigan-Wong_ reaches the greatest (Also MacQueen does) between-cluster point scatter value, we choose that method. As a result, we code the method in the following way:

```{r}
Cluster_HW <- kmeans(x = cc_general_norm,
                     centers =  5,
                     nstart = 250,
                     algorithm = "Hartigan-Wong")

```

Finally, we can see some properties of the cluster. For example, the number of observations by the cluster is:

```{r}
Cluster_HW$size
```

In this case, we can see that the classes are well balanced. Also, we can see the center of each cluster and realize some interpretations.

```{r}
palette <- c("orange4", "red2", "tomato", "darkseagreen1",  "deeppink3", "steelblue2", 
             "gold1", "wheat", "tan4", "springgreen2", "darkred", "darkseagreen3", 
             "lightskyblue", "darkorange2", "plum3", "darkgoldenrod4", "skyblue1") 

# Graph
barplot(t(Cluster_HW$centers),
        main = "Centers by cluster K-means clustering",
        xlab = "Cluster",
        beside = TRUE, 
        col = palette,
        ylim = c(0, 1.9)
        )
abline(h = 0.5, col = 3, lty = 3)
text(5, 0.55, labels= "Center > 0.5", col = "blue", adj = c(0, 0), font=2, cex= 0.8 )
legend("topright", 
       legend = colnames(Cluster_HW$centers),
       fill = palette, ncol = 2,
       cex = 0.6)
```

In cluster 1, these customers have the highest value in ONEOFF_PURCHASES_FREQUENCY. So, this group usually does the greatest purchase amount done in one-go.

Cluster 2 and 4 have the highest value in purchases_installments_frequency, so these customers used frequently their credit cards.  But cluster 2 has the highest value in PRC_FULL_PAYMENT, so this is a cluster of customers that used their card and also pay the products. 

Cluster 3 has customers with less Purchase frequency, so this group does not make frequent purchases with their credit cards.
Finally, cluster 5 has customers with less balance, balance frequency,  and purchase installments frequency.  This group is similar to group 3 because this is the group of customers that do not use frequently their credit cards, so their balance is not frequently updated.

In this way, we can segment the marketing strategy among these groups.

1) Customers with a high amount of purchases.
2) Customers that used their cards frequently.
3) Customers that no use their cards frequently. 


## **Hierachical cluster**

Using _**Ward**_ aggregation, we code the method in the following way:

```{r}

model_Ward <- hclust(dist(cc_general_norm), method = "ward.D2")

plot(model_Ward, labels = FALSE, xlab = "Hierachical cluster using Ward")
rect.hclust(model_Ward, k = 5, border = "blue")
```

```{r}
cluster <- cutree(model_Ward, k = 5)
cc_general_norm_cluster <-cbind(cc_general_norm, cluster)
library(rattle)
cc_general_center <- centers.hclust(cc_general_norm,
                                    model_Ward,
                                    nclust = 5,
                                    use.median = FALSE)
```

As in _k-means_, we can see the properties of the centers cluster in the following graph:

```{r}

barplot(t(cc_general_center),
        beside = TRUE,
        main = "Center by cluster Hierarchical Classification",
        col = palette,
        ylim = c(0, 1.9)
        )
abline(h = 0.5, col = 3, lty = 3)
text(5, 0.55, labels= "Center > 0.5", col = "blue", adj = c(0, 0), font=2, cex= 0.8 )
legend("topright", 
       legend = colnames(cc_general_center),
       fill = palette, ncol = 2,
       cex = 0.6)

```
For interpretation of the results, we can use the radar chart

* In cluster 1, the most important variables are balance and cash advance.
* In cluster 2 the most important variable is purchase frequency.
* In cluster 3 the most important variable is One-off purchase frequency.
* Cluster 4 has the highest influence on several of the variables.
* Cluster 5 has the influence of the variable PRC_Payments.

```{r}

# radar graph
center  <- as.data.frame(cc_general_center)
maximos <- apply(center,2,max)
minimos <- apply(center,2,min)
center  <- rbind(minimos,center)
center  <- rbind(maximos,center)
colnames(center) <- c("BAL", "BAL_FREQ", "PURCH", "ONEOFF_PURCH","INST_PURCH.",
                      "CASH_ADV", "PURCH._FREQ", "ONEOFF_PURCH_FREQ",
                      "PURCH_INSTALL_FREQ","CASH_ADV_FREQ", "CASH_ADV_TRX",
                      "PURCH_TRX", "CRED_LIM", "PAY", "MIN_PAY", "PRC_PAY", "TENURE")

```


```{r}
library(fmsb)
radarchart(center,
           maxmin = TRUE,
           axistype = 4,
           axislabcol = "slategray4",
           centerzero = FALSE,
           seg = 8,
           cglcol = "gray67",
           pcol= palette,
           plty = 1,
           plwd = 5,
           title = "Cluster comparation Hierarchical Classification")
det_radar   <-legend(1.5,1, legend=c("Cluster 1","Cluster 2","Cluster 3",
                                     "Cluster 4", "Cluster 5"),
                 seg.len=-1.4,
                 title="Cluster",
                 pch=21, 
                 bty="n" ,lwd=3, y.intersp=1, horiz=FALSE,
                 col= palette
                 )
```


<!--
# References
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\vspace*{-0.2in}
\noindent
-->
